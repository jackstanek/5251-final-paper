% MATH 5251 Final paper.

\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{tikz}
\usepackage[pdfusetitle]{hyperref}
\usepackage{apacite}

\textwidth = 7 in
\textheight = 9.5 in
\oddsidemargin = -0.3 in
\evensidemargin = -0.3 in
\topmargin = -0.4 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}[section]

\title{A Review of Lossless Data Compression Methods}
\author{Blargh}

\begin{document}
\maketitle

In the age of ever-expanding quantities of digital data, the ability
to store this data efficiently is more important than ever. Throughout
the history of the field of computer science, various algorithms of
varying complexities and efficacies have been created in order to
reduce the amount of space required to store arbitrary strings of
binary data. This wide variety of methods are optimized for different
purposes: some algorithms attempt to reduce the amount of space
required to store raster images; some try to optimize audio storage;
some can optimize genetic data to an extremely high degree; and still
more attempt to work well for general-purpose use. Within the scope of
this paper, I will attempt to give an overview of a handful of popular
compression schemes, including general purpose in addition to
domain-specific methods, as well as a theoretical overview of the
limits and potential benefits of compression in general. This paper
will only cover lossless data encoding: that is, alogrithms which can
reproduce a perfect copy of the input data from its output. Lossy
compression schemes exist which can achieve incredible compression
ratios, but their methods typically include psychological ``tricks''
which allows the algorithm to discard data which is less noticable to
human senses. As such, these methods are outside the scope of this
paper.

\section{Theoretical Limits to Data Compression}

Data compression has many benefits as quickly reviewed above; however,
many limitations also exist. Note that for simplicity, we will limit
our discussion of compression schemes to binary schemes, as these are
the most wide spread in applications in the real world (i.e.,
compression of computer files.) The first limitation we shall discuss
is the impossibility of a universally applicable, or ``perfect,''
compression scheme.

\begin{thm}
  For any given data compression scheme, there exists at least one
  arbitrary input data whose ``compressed'' form is not strictly
  shorter than it. That is to say, there is no compression scheme
  which can compress every potential input.
\end{thm}

\begin{proof}
  Informally, we can describe a ``perfect compression scheme'' as some
  mapping which uniquely maps given input bit-strings to output
  bit-strings which are shorter. More formally, a perfect compression
  scheme can be modeled as a bijection which maps input bit-strings to
  output bit-strings of strictly lesser length. For any
  $n \in \mathbb{N}$, let $S_n$ be the set of bit-strings of length
  $n$, and let $T_n = \bigcup_{k = 1}^{n - 1} S_k$; that is, $T_n$ is
  the ``target'' set of bit-strings which are of lengths strictly less
  than $n$. Suppose that there exists some perfect compression scheme
  $C:S_n \to T_n$. The set $S_n$ has $2^n$ elements and $T_n$ has
  $\sum_{k=1}^{n-1}2^k = 2^n - 2$ elements. Since
  $|S_n| = 2^n > 2^n - 2 = |T_n|$, by the pigeonhole principle there
  must be at least one element $t \in T_n$ such that at least two
  distinct elements in $S_n$ map to $t$ through $C$. However, $C$ is a
  bijection by definition, so this is a contradiction. Thus a perfect
  compression algorithm cannot exist for arbitrary-length bit-strings.
\end{proof}

A brief note on the implications of this result: this applies to all
lossless compression schemes, as a compression scheme must be able to
uniquely decompress a given compressed bit-string. If it cannot do
this, the compression scheme is in effect discarding the data of at
least one of its potential inputs and is then clearly not
``lossless.''

This result also raises the question of which inputs are more likely
to be compressable in general.  (That is, regardless of compression
algorithm implementation, what sorts of input can yield a higher
compression ratio?) In practice, most compression algorithms function
by finding common repeated patterns within the input data, and then
replacing each instance of this input with a shorter ``prefix code.''
Intuitively, this suggests that data with a regular structure is more
amenable to compression than more random, unpredictable data.

\begin{thm}
  The average word length of an optimal prefix code for a given source
  is greater than or equal to the entropy of that source.
\end{thm}

\begin{proof}
  A proof of this theorem can be found in \cite{Shannon}.
\end{proof}

This result places a lower bound on the \emph{efficiency} of prefix
codes in general, and suggests the sorts of data which are less
compressable, and even uncompressable (since such an input must
exist!) Since entropy of some channel $X$ with an alphabet $C$ is
defined to be $H(X) = -\sum_{c \in C} P(c)\log_2{c}$, we can
illustrate the implications of this result with a simple
example. Suppose we are attempting to compress a particular channel
$X$ which is text document in which the frequencies of every letter of
the Latin alphabet are distributed equally. (It is extremely unlikely
that a text in any human language would have this property; it is used
here to illustrate a pathological edge case in general.) Then the
average word length must be greater than
$26 * -\frac{1}{26} \log_2{\frac{1}{26}} \approx 4.7$ bits. If we were
to construct a na√Øve code to encode the Latin alphabet, for instance
$A=1, B=2,...$, we would require 5 bits for each letter. Shannon's
Noiseless Coding Theorem says this is more or less as good as we can
do, so for this case it is very difficult to actually compress our
input. Consider another document in which case one letter, let's say
$E$, occurs significantly more often than the others, for instance
$\frac{27}{52}$ of the time, relative to $\frac{1}{52}$ for every
other letter. Then by Shannon's Noiseless Coding Theorem, it is
possible to construct a prefix code with an average word length
greater than
$25 * -\frac{1}{52} \log_2{\frac{1}{52}} - \frac{27}{52}
\log_2{\frac{27}{25}} \approx 3.2$, meaning that we can compress such
this document approximately $33\%$ more efficiently than the uniform
one. This situation is somewhat contrived, but it serves to illustrate
the fundamental limits of prefix codes in general.

Prefix codes on their own are extremely useful in communication
applications, but their true power is revealed when used as a building
block within more powerful, sophisticated compression schemes.

\section{Lempel-Ziv Based Methods}

The Lempel-Ziv compression algorithm, commonly referred to as LZ77 or
LZ78, or even simply LZ for short, is one of the most widespread
compression schemes in use, especially when including its large family
of derivative methods. It was first introduced in
1977. \cite{LempelZiv} Algorithms based on the Lempel-Ziv algorithms
are relatively efficient and work well with a wide variety of inputs.

The algorithm is based on a dictionary-style approach; that is to say,
it creates a dictionary of codewords within the input data as the
algorithm progresses. Lempel-Ziv uses a ``sliding-window'' technique
to construct its dictionary. For each set of bytes it attempts to
compress, it inspects its sliding window, which extends some set
amount behind the bytes under consideration. In this way, the
algorithm builds a prefix code as it proceeds through the file. A
``prefix'' can be one or more characters long. For each point in the
input data, the algorithm consults the prefix table of prefixes
contained in the sliding window, which is a set amount of data prior
to the current point. If the current prefix is found in the prefix
table, the algorithm outputs a data triplet indicating how far back in
the window the prefix occurred, the length of the prefix, and finally
the character which comes after the prefix. This means that the very
first characters in the input data will be encoded as 1-character
prefixes. (For our very first characters in the input data, since
there can't be any prefixes in the sliding window yet, the encoded
form will consist of zero-length prefixes followed by the input
character literals.)

The basic method introduced in Lempel and Ziv's original paper is
vulnerable to some significant drawbacks. Since each individual byte
in the input can take up at most three bytes in the output, it is
possible that some pathological input data can have a compressed form
which is substantially larger, especially if the sliding window is
small. As such, this suggests a good jumping-off point for creating
improvements upon the original algorithm.

\subsection{Lempel-Ziv-Welch}

The Lempel-Ziv-Welch compression algorithm was an extension of the
Lempel-Ziv algorithm which was introduced soon after the publication
of the original LZ77. \cite{LempelZivWelch} The LZW variant is
extremely widespread in applications; for instance, one of its many
uses is as part of the GIF image format.

\subsection{DEFLATE}

Another algorithm based on the Lempel-Ziv foundation is the DEFLATE
algorithm, codified in RFC 1951. \cite{RFC1951}

\section{The Free Lossless Audio Codec}

Since knowing the structure of a given input can allow the
construction of a more efficient data compression scheme, it is
worthwhile to study some compression scheme which is specifically
designed to store a particular type of data. In this secion I will
describe the structure and function of the Free Lossless Audio Codec,
known more commonly by its acronym FLAC, a popular format for
high-fidelity audio storage.

\bibliographystyle{apacite}
\bibliography{biblio}

\end{document}
% MATH 5251 Final paper.

\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{thmtools}
\usepackage{tikz}
\usepackage[pdfusetitle]{hyperref}
\usepackage{cleveref}
\usepackage{apacite}

\textwidth = 7 in
\textheight = 9.5 in
\oddsidemargin = -0.3 in
\evensidemargin = -0.3 in
\topmargin = -0.4 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\declaretheorem[numberwithin=section]{Theorem}

\title{A Review of Lossless Data Compression Methods}
\author{5251 Student}

\begin{document}
\maketitle

In the age of ever-expanding quantities of digital data, the ability
to store this data efficiently is more important than ever. Throughout
the history of the field of computer science, algorithms of varying
complexities and efficacies have been created in order to reduce the
amount of space required to store arbitrary strings of binary
data. This wide variety of methods are optimized for different
purposes: some algorithms attempt to reduce the amount of space
required to store raster images; some try to optimize audio storage;
some can optimize genetic data to an extremely high degree; and still
more attempt to work well for general-purpose use. Within the scope of
this paper, I will attempt to give an overview of a handful of popular
compression schemes, including general purpose in addition to
domain-specific methods, as well as a theoretical overview of the
limits and potential benefits of compression in general. This paper
will only cover lossless data encoding: that is, alogrithms which can
reproduce a perfect copy of the input data from its output. Lossy
compression schemes exist which can achieve incredible compression
ratios, but their methods typically include psychological ``tricks''
which allows the algorithm to discard data which is less noticable to
human senses. As such, these methods are outside the scope of this
paper.

\section{Theoretical Limits to Data Compression}

Data compression has many benefits as quickly reviewed above; however,
many limitations also exist. Note that for simplicity, we will limit
our discussion of compression schemes to binary schemes, as these are
the most wide spread in applications in the real world (i.e.,
compression of computer files.) The first limitation we shall discuss
is the impossibility of a universally applicable, or ``perfect,''
compression scheme.

\begin{Theorem}
  \label{imperfect-compression}
  For any given data compression scheme, there exists at least one
  arbitrary input data whose ``compressed'' form is not strictly
  shorter than it. That is to say, there is no compression scheme
  which can compress every potential input.
\end{Theorem}

\begin{proof}
  Informally, we can describe a ``perfect compression scheme'' as some
  mapping which uniquely maps given input bit-strings to output
  bit-strings which are shorter. More formally, a perfect compression
  scheme can be modeled as a bijection which maps input bit-strings to
  output bit-strings of strictly lesser length. For any
  $n \in \mathbb{N}$, let $S_n$ be the set of bit-strings of length
  $n$, and let $T_n = \bigcup_{k = 1}^{n - 1} S_k$; that is, $T_n$ is
  the ``target'' set of bit-strings which are of lengths strictly less
  than $n$. Suppose that there exists some perfect compression scheme
  $C:S_n \to T_n$. The set $S_n$ has $2^n$ elements and $T_n$ has
  $\sum_{k=1}^{n-1}2^k = 2^n - 2$ elements. Since
  $|S_n| = 2^n > 2^n - 2 = |T_n|$, by the pigeonhole principle there
  must be at least one element $t \in T_n$ such that at least two
  distinct elements in $S_n$ map to $t$ through $C$. However, $C$ is a
  bijection by definition, so this is a contradiction. Thus a perfect
  compression algorithm cannot exist for arbitrary-length bit-strings.
\end{proof}

A brief note on the implications of this result: this applies to all
lossless compression schemes, as a compression scheme must be able to
uniquely decompress a given compressed bit-string. If it cannot do
this, the compression scheme is in effect discarding the data of at
least one of its potential inputs and is then clearly not
``lossless.''

This result also raises the question of which inputs are more likely
to be compressable in general.  (That is, regardless of compression
algorithm implementation, what sorts of input can yield a higher
compression ratio?) In practice, most compression algorithms function
by finding common repeated patterns within the input data, and then
replacing each instance of this input with a shorter ``prefix code.''
Intuitively, this suggests that data with a regular structure is more
amenable to compression than more random, unpredictable data.

\begin{Theorem}
  \label{snct}
  The average word length of an optimal prefix code for a given source
  is greater than or equal to the entropy of that source.
\end{Theorem}

\begin{proof}
  A proof of this theorem can be found in \cite{Shannon}.
\end{proof}

This result places a lower bound on the \emph{efficiency} of prefix
codes in general, and suggests the sorts of data which are less
compressable, and even uncompressable (since such an input must
exist!) Since entropy of some channel $X$ with an alphabet $C$ is
defined to be $H(X) = -\sum_{c \in C} P(c)\log_2{c}$, we can
illustrate the implications of this result with a simple
example. Suppose we are attempting to compress a particular channel
$X$ which is text document in which the frequencies of every letter of
the Latin alphabet are distributed equally. (It is extremely unlikely
that a text in any human language would have this property; it is used
here to illustrate a pathological edge case in general.) Then the
average word length must be greater than
$26 * -\frac{1}{26} \log_2{\frac{1}{26}} \approx 4.7$ bits. If we were
to construct a na√Øve code to encode the Latin alphabet, for instance
$A=1, B=2,...$, we would require 5 bits for each letter. Shannon's
Noiseless Coding Theorem says this is more or less as good as we can
do, so for this case it is very difficult to actually compress our
input. Consider another document in which case one letter, let's say
$E$, occurs significantly more often than the others, for instance
$\frac{27}{52}$ of the time, relative to $\frac{1}{52}$ for every
other letter. Then by Shannon's Noiseless Coding Theorem, it is
possible to construct a prefix code with an average word length
greater than
$25 * -\frac{1}{52} \log_2{\frac{1}{52}} - \frac{27}{52}
\log_2{\frac{27}{25}} \approx 3.2$, meaning that we can compress such
this document approximately $33\%$ more efficiently than the uniform
one. This situation is somewhat contrived, but it serves to illustrate
the fundamental limits of prefix codes in general.

Prefix codes on their own are extremely useful in communication
applications, but their true power is revealed when used as a building
block within more powerful, sophisticated compression schemes.

\section{Lempel-Ziv Based Methods}

The Lempel-Ziv compression algorithm, commonly referred to as LZ77 or
LZ78, or even simply LZ for short, is one of the most widespread
compression schemes in use, especially when including its large family
of derivative methods. It was first introduced in
1977. \cite{LempelZiv} Algorithms based on the Lempel-Ziv algorithms
are relatively efficient and work well with a wide variety of inputs.

The algorithm is based on a dictionary-style approach; that is to say,
it creates a dictionary of codewords within the input data as the
algorithm progresses. Lempel-Ziv uses a ``sliding-window'' technique
to construct its dictionary. For each set of bytes it attempts to
compress, it inspects its sliding window, which extends some set
amount behind the bytes under consideration. In this way, the
algorithm builds a prefix code as it proceeds through the file. A
``prefix'' can be one or more characters long. For each point in the
input data, the algorithm consults the prefix table of prefixes
contained in the sliding window, which is a set amount of data prior
to the current point. If the current prefix is found in the prefix
table, the algorithm outputs a data triplet indicating how far back in
the window the prefix occurred, the length of the prefix, and finally
the character which comes after the prefix. This means that the very
first characters in the input data will be encoded as 1-character
prefixes. (For our very first characters in the input data, since
there can't be any prefixes in the sliding window yet, the encoded
form will consist of zero-length prefixes followed by the input
character literals.)

The basic method introduced in Lempel and Ziv's original paper is
vulnerable to some significant drawbacks. For instance, since each
individual byte in the input can take up at most three bytes in the
output, it is possible that some pathological input data can have a
compressed form which is substantially larger, especially if the
sliding window is small. (Clearly, a highly entropic input is much
more likely to have fewer recurring prefixes within any given sliding
window. Thus a prefix code based on such an output would have a higher
average word length, which is consistent with our result from
\Cref{snct}.) Another drawback of the LZ method is its performance;
searching the sliding window for matches, as well as updating the
window prefix list, can be an expensive operation. As such, these
drawbacks suggest a good jumping-off point for creating improvements
upon the original algorithm.

\subsection{Lempel-Ziv-Welch}

One such improvement was the Lempel-Ziv-Welch compression algorithm,
commonly referred to as LZW. It was created as an extension of the
Lempel-Ziv algorithm and was introduced soon after the publication of
the original LZ77. The LZW variant is extremely widespread in
applications; for instance, one of its many uses is as part of the GIF
image format. LZW's improvements upon the LZ algorithm are fairly
straightforward, but they allow for significant improvements to the
simplicity of implementation. According to the author, the LZW
algorithm can take operate at a speed of three clock cycles (CPU
instructions) per input symbol. \cite{LempelZivWelch}

LZW is able to achieve this speed by using a single symbol table for
the entire input, adaptively building up as the algorithm processes
it. Instead of using a sliding window, LZW instead begins by
initializing a string table with every single-character prefix. As it
parses the input, it uses a greedy algorithm to match strings from the
input to the strings already seen in the table. It does this by
checking if some prefix in the input has already been seen in the
table; if so, it appends the immediately next character from the input
to the prefix and queries the string table again. If, on the other
hand, the appended prefix is \emph{not} found, the algorithm outputs
the code of the prefix \emph{without} the next character appended (the
code for which is retrieved from the string table) and then adding the
appended prefix to the string table. The prefix is set to the last
character of the prefix and the process repeats.

As we can see, the entire algorithm can be succinctly summarized in a
few sentences and can also be (hopefully) easily understood. This
characteristic makes it useful for implementers, and also has benefits
in terms of performance. Since it uses a very simple process for
encoding, it is able to greatly reduce the number of clock cycles
needed for encoding or decoding each character. In addition, it is
also simple to implement performance-enhancing features, such as using
a tree data structure with hashing to store string prefixes, buffering
the input data to conserve memory, and so forth. Drawbacks of the LZW
method include lower compression ratios than other, more advanced
methods. For instance, the author claims a ratio of 1.8 on English
text, compared to a 2.4 ratio achieved by more sophisticated
algorithms. \cite{LempelZivWelch}

The LZ family of compression codes is massive, and LZW is just one
single member. Possibly even more widespread is the DEFLATE algorithm,
used in many compressed file formats.

\subsection{DEFLATE}

Another algorithm based on the Lempel-Ziv foundation is the DEFLATE
format, codified in RFC 1951. \cite{RFC1951} DEFLATE is not
``standardized'' by any recognized governing body, though it is very
widely used and has become a de facto standard by virtue of its use in
the \texttt{gzip} compression utility, as well as in the ubiquitous
\texttt{zlib} library. RFC 1951 specifies the technical minutia of the
implementation of the file format, as well as its algorithm itself. As
such, it is not useful to describe the full file format within this
paper, only the algorithm will be outlined.

In general, compression proceeds as follows: for a given block of data
withint the input, the algorithm constructs a Huffman prefix coding
for the input alphabet. Information about the prefix coding is then
stored within the output block header, which contains metadata about the
compressed output. DEFLATE supports ``fixed'' and ``dynamic'' Huffman
codes. Fixed Huffman codes are defined in the DEFLATE specification
and can be simply hard-coded in the compressor/decompressor
software. Dynamic Huffman codes, on the other hand, are created by the
compressor and their information is serialized and stored in the block
header. The compressed data is then run through an implementation of
the LZ77 algorithm from earlier.

DEFLATE allows us to reap the benefits both of the the LZ77 algorithm
as well as the more basic Huffman prefix coding scheme, all while
using a more-or-less standardized file format for easy interchange.

\section{The Free Lossless Audio Codec}

Since knowing the structure of a given input can allow the
construction of a more efficient data compression scheme, it is
worthwhile to study some compression scheme which is specifically
designed to store a particular type of data. In this secion I will
describe the structure and function of the Free Lossless Audio Codec,
known more commonly by its acronym FLAC, a popular format for
high-fidelity audio storage. \cite{flac}

Since FLAC is designed exclusively for storing high-fidelity audio
data, it is able to exploit characteristics specific to sound in order
to achieve a better compression ratio than a simple general purpose
compression scheme would be able to. For instance, FLAC's designers
realize that stereo sound streams often correlate very closely. As
such, FLAC allows the compressor to encode input in multiple ways;
left and right can be encoded independently, or as ``mid'' and
``side'' channels, where the mid channel is an average of the left and
right channels and the side is the difference of the two. For highly
correlated left/right channels, this encoding can result in
significant lossless compression gains.

The primary means of compression, however, is through model/residual
coding. For each block of raw input, the FLAC compressor tries to fit
the signal to a parameterized function, such as a polynomial for
instance, and then stores the function. It then computes the
residuals, or the differences between the raw audio signal and the
parameterized function at each sample. This residual data has a
particular distribution which is efficiently stored using a specific
form of Huffman codes, called Rice codes. \cite{flac}

The FLAC specification is of course much more detailed than this
high-level overview, but a brief description of FLAC's methods is more
than sufficient to demonstrate how exploiting characteristics of a
particular data domain can lead to huge increases in compression
efficiency.

\section{Conclusion}

Data compression methods can be used to reduce the amount of space
required to store digital data. However, tradeoffs have to be made
when compressing since it is impossible for an algorithm to be able to
compress any arbitrary input; some input must result in an output of
equal length. In addition, the compressability of an input is highly
dependent on its entropy. Many different compression algorithms exist,
the most common of which belong to the Lempel-Ziv family. Finally,
many domain-specific algorithms exist to gain extra efficiency when
applied to their particular domain. FLAC is one such algorithm which
is used for lossless audio storage and transmission.

\bibliographystyle{apacite}
\bibliography{biblio}

\end{document}
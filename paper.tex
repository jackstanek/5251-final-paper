% MATH 5251 Final paper.

\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{tikz}
\usepackage[pdfusetitle]{hyperref}

\textwidth = 7 in
\textheight = 9.5 in
\oddsidemargin = -0.3 in
\evensidemargin = -0.3 in
\topmargin = -0.4 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}[section]

\title{A Review of Lossless Data Compression Methods}
\author{Anonymous}

\begin{document}
\maketitle

In the age of ever-expanding quantities of digital data, the ability
to store this data efficiently is more important than ever. Throughout
the history of the field of computer science, various algorithms of
varying complexities and efficacies have been created in order to
reduce the amount of space required to store arbitrary strings of
binary data. This wide variety of methods are optimized for different
purposes: some algorithms attempt to reduce the amount of space
required to store raster images; some try to optimize audio storage;
some can optimize genetic data to an extremely high degree; and still
more attempt to work well for general-purpose use. Within the scope of
this paper, I will attempt to give an overview of a handful of popular
compression schemes, including general purpose in addition to
domain-specific methods, as well as a theoretical overview of the
limits and potential benefits of compression in general. This paper
will only cover lossless data encoding: that is, alogrithms which can
reproduce a perfect copy of the input data from its output. Lossy
compression schemes exist which can achieve incredible compression
ratios, but their methods typically include psychological ``tricks''
which allows the algorithm to discard data which is less noticable to
human senses. As such, these methods are outside the scope of this
paper.

\section{Theoretical Limits to Data Compression}

Data compression has many benefits as quickly reviewed above; however,
many limitations also exist. Note that for simplicity, we will limit
our discussion of compression schemes to binary schemes, as these are
the most wide spread in applications in the real world (i.e.,
compression of computer files.) The first limitation we shall discuss
is the impossibility of a universally applicable, or ``perfect,''
compression scheme.

\begin{thm}
  For any given data compression scheme, there exists at least one
  arbitrary input data whose ``compressed'' form is not strictly
  shorter than it. That is to say, there is no compression scheme
  which can compress every potential input.
\end{thm}

\begin{proof}
  Informally, we can describe a ``perfect compression scheme'' as some
  mapping which uniquely maps given input bit-strings to output
  bit-strings which are shorter. More formally, a perfect compression
  scheme can be modeled as a bijection which maps input bit-strings to
  output bit-strings of strictly lesser length. For any
  $n \in \mathbb{N}$, let $S_n$ be the set of bit-strings of length
  $n$, and let $T_n = \bigcup_{k = 1}^{n - 1} S_k$; that is, $T_n$ is
  the ``target'' set of bit-strings which are of lengths strictly less
  than $n$. Suppose that there exists some perfect compression scheme
  $C:S_n \to T_n$. The set $S_n$ has $2^n$ elements and $T_n$ has
  $\sum_{k=1}^{n-1}2^k = 2^n - 2$ elements. Since
  $|S_n| = 2^n > 2^n - 2 = |T_n|$, by the pigeonhole principle there
  must be at least one element $t \in T_n$ such that at least two
  distinct elements in $S_n$ map to $t$ through $C$. However, $C$ is a
  bijection by definition, so this is a contradiction. Thus a perfect
  compression algorithm cannot exist for arbitrary-length bit-strings.
\end{proof}

A brief note on the implications of this result: this applies to all
lossless compression schemes, as a compression scheme must be able to
uniquely decompress a given compressed bit-string. If it cannot do
this, the compression scheme is in effect discarding the data of at
least one of its potential inputs and is then clearly not
``lossless.''

This result also raises the question of which inputs are more likely
to be compressable in general.  (That is, regardless of compression
algorithm implementation, what sorts of input can yield a higher
compression ratio?) In practice, most compression algorithms function
by finding common repeated patterns within the input data, and then
replacing each instance of this input with a shorter ``prefix code.''
Intuitively, this suggests that data with a regular structure is more
amenable to compression than more random, unpredictable data.

\begin{thm}
  The average word length of an optimal prefix code for a given source
  is greater than or equal to the entropy of that source.
\end{thm}

\begin{proof}
  A proof of this theorem can be found in \cite{Shannon}.
\end{proof}

This result places a lower bound on the \emph{efficiency} of prefix
codes in general, and suggests the sorts of data which are less
compressable, and even uncompressable (since such an input must
exist!) Since entropy of some channel $X$ with an alphabet $C$ is
defined to be $H(X) = -\sum_{c \in C} P(c)\log_2{c}$, we can
illustrate the implications of this result with a simple
example. Clearly for a channel using binary alphabet (i.e.,
$C = \{0, 1\}$) with a uniform distribution of 1s and 0s, the entropy
of this channel is $H(X) = -2 * \frac{1}{2}\log_2{\frac{1}{2}} = 1$,
so some optimal prefix code must have an average word length greater
than or equal to 1; obviously, the simplest prefix code to encode
$\{0, 1\}$ is $\{0, 1\}$ itself. Thus, a this channel is
incompressible.

\section{Lempel-Ziv Based Methods}

The Lempel-Ziv compression algorithm, or simply LZ for short, is one
of the most widespread compression schemes in use, when including its
large family of derivative methods. It was first introduced in 1977 in
\cite{LempelZiv}.

\section{The Free Lossless Audio Codec}

Since knowing the structure of a given input can allow the
construction of a more efficient data compression scheme, it is
worthwhile to study some compression scheme which is specifically
designed to store a particular type of data. In this section I will
describe the structure and function of the Free Lossless Audio Codec,
known more commonly by its acronym FLAC, a popular format for
high-fidelity audio storage.

\bibliography{biblio}{}
\bibliographystyle{plain}

\end{document}
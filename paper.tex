% MATH 5251 Final paper.

\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{thmtools}
\usepackage{tikz}
\usepackage[pdfusetitle]{hyperref}
\usepackage{cleveref}
\usepackage{apacite}

\textwidth = 7 in
\textheight = 9.5 in
\oddsidemargin = -0.3 in
\evensidemargin = -0.3 in
\topmargin = -0.4 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\declaretheorem[numberwithin=section]{Theorem}

\title{A Review of Lossless Data Compression Methods}
\author{Jack Stanek}
\date{December 10, 2018}

\begin{document}
\maketitle

In the age of ever-expanding quantities of digital data, the ability
to store this data efficiently is more important than ever. Throughout
the history of the field of computer science, algorithms of varying
complexities and efficacies have been created in order to reduce the
amount of space required to store arbitrary strings of binary
data. This wide variety of methods are optimized for different
purposes: some algorithms attempt to reduce the amount of space
required to store raster images; some try to optimize audio storage;
some can optimize genetic data to an extremely high degree; and still
more attempt to work well for general-purpose use. Within the scope of
this paper, I will attempt to give an overview of a handful of popular
compression schemes, including general purpose in addition to
domain-specific methods, as well as a theoretical overview of the
limits and potential benefits of compression in general. This paper
will only cover lossless data encoding: that is, alogrithms which can
reproduce a perfect copy of the input data from its output. Lossy
compression schemes exist which can achieve incredible compression
ratios, but their methods typically include psychological ``tricks''
which allows the algorithm to discard data which is less noticable to
human senses. As this is a mathematics paper and not a psychology
paper, these methods are outside the scope of this document.

\section{Theoretical Limits to Data Compression}

Data compression has many benefits as quickly reviewed above; however,
many limitations also exist. Note that for simplicity, we will limit
our discussion of compression schemes to binary schemes, as these are
the most widespread in applications in the real world (i.e.,
compression of computer files.) The first limitation we shall discuss
is the impossibility of a universally applicable, or ``perfect,''
compression scheme.

\begin{Theorem}
  \label{imperfect-compression}
  For any given data compression scheme, there exists at least one
  arbitrary input data whose ``compressed'' form is not strictly
  shorter than it. That is to say, there is no compression scheme
  which can compress every potential input.
\end{Theorem}

\begin{proof}
  Informally, we can describe a ``perfect compression scheme'' as some
  mapping which uniquely maps given input bit-strings to output
  bit-strings which are shorter. More formally, a perfect compression
  scheme can be modeled as a bijection which maps input bit-strings to
  output bit-strings of strictly lesser length. For any
  $n \in \mathbb{N}$, let $S_n$ be the set of bit-strings of length
  $n$, and let $T_n = \bigcup_{k = 1}^{n - 1} S_k$; that is, $T_n$ is
  the ``target'' set of bit-strings which are of lengths strictly less
  than $n$. Suppose that there exists some perfect compression scheme
  $C:S_n \to T_n$. The set $S_n$ has $2^n$ elements and $T_n$ has
  $\sum_{k=1}^{n-1}2^k = 2^n - 2$ elements. Since
  $|S_n| = 2^n > 2^n - 2 = |T_n|$, by the pigeonhole principle there
  must be at least one element $t \in T_n$ such that at least two
  distinct elements in $S_n$ map to $t$ through $C$. However, $C$ is a
  bijection by definition, so this is a contradiction. Thus a perfect
  compression algorithm cannot exist for arbitrary-length bit-strings.
\end{proof}

A brief note on the implications of this result: this applies to all
lossless compression schemes, as a compression scheme must be able to
uniquely decompress a given compressed bit-string. If it cannot do
this, the compression scheme is in effect discarding the data of at
least one of its potential inputs and is then clearly not
``lossless.''

This result also raises the question of which inputs are more likely
to be compressable in general.  (That is, regardless of compression
algorithm implementation, what sorts of input can yield a higher
compression ratio?) In practice, most compression algorithms function
by finding common repeated patterns within the input data, and then
replacing each instance of this input with a shorter ``prefix code.''
Intuitively, this suggests that data with a regular structure is more
amenable to compression than more random, unpredictable data.

\begin{Theorem}
  \label{snct}
  The average word length of an optimal prefix code for a given source
  is greater than or equal to the entropy of that source.
\end{Theorem}

\begin{proof}
  A proof of this theorem can be found in \cite{Shannon}.
\end{proof}

This result places a lower bound on the \emph{efficiency} of prefix
codes in general, and suggests the sorts of data which are less
compressable, and even uncompressable (since such an input must
exist!) Since entropy of some channel $X$ with an alphabet $C$ is
defined to be $H(X) = -\sum_{c \in C} P(c)\log_2{c}$, we can
illustrate the implications of this result with a simple
example. Suppose we are attempting to compress some arbitrary channel,
$X$, which is text document in which the frequencies of every letter
of the Latin alphabet are distributed equally. (It is extremely
unlikely that a text in any human language would have this property;
it is used here to illustrate a pathological edge case in general.)
Then the average word length must be greater than
$26 * -\frac{1}{26} \log_2{\frac{1}{26}} \approx 4.7$ bits. If we were
to construct a na√Øve code to encode the Latin alphabet, for instance
$A=1, B=2,...$, we would require 5 bits for each letter. Shannon's
Noiseless Coding Theorem says this is more or less as good as we can
do, so for this case it is very difficult to actually compress our
input. Consider another document in which case one letter, let's say
$E$, occurs significantly more often than the others, for instance
$\frac{27}{52}$ of the time, relative to $\frac{1}{52}$ for every
other letter. Then by Shannon's Noiseless Coding Theorem, it is
possible to construct a prefix code with an average word length
greater than
$25 * -\frac{1}{52} \log_2{\frac{1}{52}} - \frac{27}{52}
\log_2{\frac{27}{25}} \approx 3.2$, meaning that we can compress such
this document approximately $33\%$ more efficiently than the uniform
one. This situation is somewhat contrived, but it serves to illustrate
the fundamental limits of prefix codes in general.

Prefix codes on their own are extremely useful in communication
applications, but their true power is revealed when used as a building
block within more powerful, sophisticated compression schemes.

\section{Lempel-Ziv Based Methods}

The Lempel-Ziv compression algorithm, introduced in two variations in
1977 and 1978, is one of the most widespread compression schemes in
use, especially when including its large family of derivative
methods. Algorithms based on the Lempel-Ziv algorithms are relatively
efficient and work well with a wide variety of inputs.

The algorithm is based on a dictionary-style approach; that is to say,
it creates a dictionary of codewords within the input data as the
algorithm progresses. The LZ77 variant uses a ``sliding-window''
technique to construct its dictionary as it compresses. \cite{lz77}
For each set of bytes it attempts to compress, it inspects its sliding
window, which extends some set amount behind the bytes under
consideration. In this way, the algorithm builds a prefix code as it
proceeds through the file. A ``prefix'' can be one or more characters
long. For each point in the input data, the algorithm consults the
prefix table of prefixes contained in the sliding window, which is a
set amount of data prior to the current point. If the current prefix
is found in the prefix table, the algorithm outputs a 3-tuple
indicating how far back in the window the prefix occurred, the length
of the prefix, and finally the character which comes after the
prefix. This means that the very first characters in the input data
will be encoded as 1-character prefixes. (For our very first
characters in the input data, since there can't be any prefixes in the
sliding window yet, the encoded form will consist of zero-length
prefixes followed by the input character literals.)

To illustrate: suppose we want to encode the laughing string
``hahahahaha'' using the LZ77 algorithm with an 8-character window. We
would begin by encoding the first two (unique) characters as
\texttt{(0, 0, 'h')} and \texttt{(0, 0, 'a')}, respectively. Then, the
rest of the word can be encoded as \texttt{(2, 8, '\textbackslash
  0')}, where \texttt{'\textbackslash 0'} represents the ASCII null
character. The resultant output can be stored in 9 bytes, compared to
the 11 required by the original input string. This example also
illustrates how the algorithm can use a powerful implementation or
run-length encoding; LZ77 can represent prefixes which extend past the
end of the sliding window by ``feeding'' output characters back into
the decoder, thus allowing repetative cyclic prefixes to be
represented compactly.

% To illustrate: suppose we are to encode the tongue-twister ``can you
% can a can as a can man can can a can'' using LZ77, with a window of
% length 8. The string is 44 characters long, thus requiring 44 bytes to
% store. In its implementation, the algorithm implicitly holds a
% windows-worth full of null-character bytes (represented in text as
% \texttt{\textbackslash 0 or 0x00}) sets the current character to the
% first character of the input. Since, in this case, the first 7
% characters of the input are all unique, and as such they cannot be
% represented by duplicate prefixes from earlier in the text (as there
% is no ``earlier text'' which matches it.) Thus each of these first
% bytes is emitted as \texttt{(0, 1, character)}. By this point, the
% current prefix has advanced to the second space character. Since there
% is a space less than 8 characters before this, the next character
% (\texttt{'c'}) is appended to the current prefix. Since the prefix
% \texttt{' c'} is not found in the window, the algorithm emits
% \texttt{(4, 1, 'c')}. The next prefix is set to \texttt{'a'} and the
% process repeats. This time, the prefix \texttt{'an '} can be found in
% the window, so the algorithm proceeds up to the following \texttt{'c'}
% and then emits \texttt{(7, 3, 'c')}. The next prefix is set to
% \texttt{'a'} and the algorithm continues. For brevity, the remaining
% steps will not be listed, but the compressed version contains 17
% 3-tuples, requiring 53 bytes to store. This example shows not only
% that any compression algorithm has incompressable inputs, but also
% that the design of an algorithm can affect which inputs are
% incompressable. (In this case, the first few bytes are encoded as
% three-byte triples, whereas later bytes are encoded as pure
% prefixes. Since this example is short, the cost incurred by the first
% bytes is too high to be overcome by the later savings.)

The LZ78 algorithm is functionally equivalent to the LZ77 algorithm,
but it forgoes the sliding window altogether, and instead uses a
``linked dictionary''. \cite{lz78} As the compressor processes the
input data, it maintains a matching index to longest prefix already
stored in the dictionary. If the prefix created by appending the
current character to the last matched prefix is not found in the
dictionary, the algorithm stores the character at the next available
index slot along with a pointer to the index of the original prefix,
and outputs the index of the longest matching prefix followed by the
character. Algorithmically, this method is equivalent to LZ77.

The basic method introduced in Lempel and Ziv's original paper is
vulnerable to some significant drawbacks. For instance, since each
individual byte in the input can take up at most three bytes in the
output, it is possible that some pathological input data can have a
compressed form which is substantially larger, especially if the
sliding window is small. (Clearly, a highly entropic input is much
more likely to have fewer recurring prefixes within any given sliding
window. Thus a prefix code based on such an output would have a higher
average word length, which is consistent with our result from
\Cref{snct}.) Another drawback of the LZ method is its performance;
searching the sliding window for matches, as well as updating the
window prefix list, can be an expensive operation. As such, these
drawbacks suggest a good jumping-off point for creating improvements
upon the original algorithm.

\subsection{Lempel-Ziv-Welch}

One such improvement was the Lempel-Ziv-Welch compression algorithm,
commonly referred to as LZW. It was created as an extension of the
Lempel-Ziv algorithm and was introduced soon after the publication of
the original LZ77. The LZW variant is extremely widespread in
applications; for instance, one of its many uses is as part of the GIF
image format. LZW's improvements upon the LZ algorithm are fairly
straightforward, but they allow for significant improvements to the
simplicity of implementation. According to the author, the LZW
algorithm can take operate at a speed of three clock cycles (CPU
instructions) per input symbol. \cite{LempelZivWelch}

LZW is able to achieve this speed by using a single symbol table for
the entire input, adaptively building up as the algorithm processes
it. Instead of using a sliding window, LZW instead begins by
initializing a string table with every single-character prefix. As it
parses the input, it uses a greedy algorithm to match strings from the
input to the strings already seen in the table. It does this by
checking if some prefix in the input has already been seen in the
table; if so, it appends the immediately next character from the input
to the prefix and queries the string table again. If, on the other
hand, the appended prefix is \emph{not} found, the algorithm outputs
the code of the prefix \emph{without} the next character appended (the
code for which is retrieved from the string table) and then adds the
appended prefix to the string table. The current prefix is finally set
to the last character of the prefix and the process repeats.

As we can see, the entire algorithm can be succinctly summarized in a
few sentences and can also be (hopefully) easily understood. This
characteristic makes it useful for implementers, and also has benefits
in terms of performance. Since it uses a very simple process for
encoding, it is able to greatly reduce the number of clock cycles
needed for encoding or decoding each character. In addition, it is
also simple to implement performance-enhancing features, such as using
a tree data structure with hashing to store string prefixes, buffering
the input data to conserve memory, and so forth. Drawbacks of the LZW
method include lower compression ratios than other, more advanced
methods. For instance, the author claims a ratio of 1.8 on English
text, compared to a 2.4 ratio achieved by more sophisticated
algorithms. \cite{LempelZivWelch}

The LZ family of compression codes is massive, and LZW is just one
single member. Possibly even more widespread is the DEFLATE algorithm,
used in many compressed file formats.

\subsection{DEFLATE}

An exmaple of an efficient implementation of a compression algorithm
can be seen in the DEFLATE format, codified in the RFC 1951
memo. \cite{RFC1951} DEFLATE is not ``standardized'' by any recognized
governing body, though it is very widely used and has become a de
facto standard by virtue of its use in the \texttt{gzip} compression
utility, as well as in the ubiquitous \texttt{zlib} library. RFC 1951
specifies the technical minutia of the implementation of the file
format, as well as its algorithm itself. As such, it is not useful to
describe the full file format within this paper, and so only the
algorithm will be outlined.

Briefly, compression proceeds as follows: for a given block of data
within the input, the algorithm constructs a Huffman prefix coding for
the input alphabet. Information about the prefix coding is then stored
within the output block header, which contains metadata about the
compressed output. DEFLATE supports ``fixed'' and ``dynamic'' Huffman
codes. Fixed Huffman codes are defined in the DEFLATE specification
and can be simply hard-coded in the compressor/decompressor
software. Dynamic Huffman codes, on the other hand, are created by the
compressor and their information is serialized and stored in the block
header. The compressed data is then run through an implementation of
the LZ77 algorithm. As such, this combination of prefix coding with a
powerful compression scheme allows compression tools to squeeze every
last bit of possible compression out of their data.

DEFLATE allows us to reap the benefits both of the the LZ77 algorithm
as well as the more basic Huffman prefix coding scheme, all while
using a more-or-less standardized file format for easy interchange.

\section{The Free Lossless Audio Codec}

Since knowing the structure of a given input can allow the
construction of a more efficient data compression scheme, it is
worthwhile to study some compression scheme which is specifically
designed to store a particular type of data. In this section I will
describe the structure and function of the Free Lossless Audio Codec,
known more commonly by its acronym FLAC, a popular format for
high-fidelity audio storage. \cite{flac}

Since FLAC is designed exclusively for storing high-fidelity audio
data, it is able to exploit characteristics specific to sound in order
to achieve a better compression ratio than a simple general purpose
compression scheme would be able to. For instance, FLAC's designers
realize that stereo sound streams often correlate very closely. As
such, FLAC allows the compressor to encode input in multiple ways;
left and right can be encoded independently, or as ``mid'' and
``side'' channels, where the mid channel is an average of the left and
right channels and the side is the difference of the two. For highly
correlated left/right channels, this encoding can result in
significant lossless compression gains.

The primary means of compression, however, is through model/residual
coding. For each block of raw input, the FLAC compressor tries to fit
the signal to a parameterized function, typically a polynomial, and
then stores the function. It then computes the residuals, or the
differences between the raw audio signal and the parameterized
function at each sample. This residual data has a particular
distribution which is efficiently stored using a specific form of
Huffman codes, called Rice codes. \cite{flac}

The FLAC specification is of course much more detailed than this
high-level overview, but a brief description of FLAC's methods is more
than sufficient to demonstrate how exploiting characteristics of a
particular data domain can lead to huge increases in compression
efficiency.

\section{Conclusion}

Data compression methods can be used to reduce the amount of space
required to store digital data. However, tradeoffs have to be made
when compressing since it is impossible for an algorithm to be able to
compress any arbitrary input; some input must result in an output of
equal length. In addition, the compressability of an input is highly
dependent on its entropy. Many different compression algorithms exist,
the most common of which belong to the Lempel-Ziv family. Finally,
many domain-specific algorithms exist to gain extra efficiency when
applied to their particular domain. FLAC is one such algorithm which
is used for lossless audio storage and transmission.

\bibliographystyle{apacite}
\bibliography{biblio}

\end{document}